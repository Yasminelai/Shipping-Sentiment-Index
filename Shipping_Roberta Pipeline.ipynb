{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8076ca4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"transformers>=4.41.0\" \"datasets>=2.19.0\" \"accelerate>=0.30.0\" \"scikit-learn>=1.3.0\" \"pandas>=2.0.0\" \"openpyxl>=3.1.0\" --upgrade\n",
    "\n",
    "import os, random, numpy as np, torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d465407",
   "metadata": {},
   "source": [
    "## DAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# === 5. Training arguments ===\n",
    "dapt_output_dir = \"./mlm_roberta_shipping\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=dapt_output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir=f\"{dapt_output_dir}/logs\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# === 6. Load model ===\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_ckpt)\n",
    "\n",
    "# === 6.1 Define compute_metrics for MLM accuracy ===\n",
    "import numpy as np\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    mask = labels != -100  # Ignore padding positions\n",
    "    correct = (predictions == labels) & mask\n",
    "    accuracy = correct.sum() / mask.sum()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# === 7. Trainer ===\n",
    "small_eval_dataset = tokenized[\"test\"].select(range(500))\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "initial_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "print(\"\\n🔍 Step 0 Evaluation (before training):\")\n",
    "print(initial_metrics)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(dapt_output_dir)\n",
    "tokenizer.save_pretrained(dapt_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f56ad",
   "metadata": {},
   "source": [
    "## TAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. TAPT using task-relevant unlabeled data ===\n",
    "finnews_df = pd.read_csv(os.path.join(folder_path, 'finnews.csv'))\n",
    "finnews_df = finnews_df[[\"clean_text\"]].dropna().rename(columns={\"clean_text\": \"text\"})\n",
    "dataset = Dataset.from_pandas(finnews_df)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "def tapt_tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized = dataset.map(tapt_tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# === 10.1  ===\n",
    "small_eval_dataset = tokenized[\"test\"].select(range(500))\n",
    "\n",
    "# === 10.2 TAPT TrainingArguments ===\n",
    "tapt_output_dir = \"./mlm_roberta_shipping_tapt\"\n",
    "tapt_args = TrainingArguments(\n",
    "    output_dir=tapt_output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir=f\"{tapt_output_dir}/logs\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# === 10.3 Load DAPT model as base ===\n",
    "model = AutoModelForMaskedLM.from_pretrained(dapt_output_dir)\n",
    "\n",
    "# === 10.4 Accuracy compute_metrics function ===\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    mask = labels != -100\n",
    "    correct = (predictions == labels) & mask\n",
    "    accuracy = correct.sum() / mask.sum()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# === 10.5 TAPT Trainer ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=tapt_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "initial_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "print(\"\\n🔍 Step 0 Evaluation (before training):\")\n",
    "print(initial_metrics)\n",
    "trainer.train()\n",
    "trainer.save_model(tapt_output_dir)\n",
    "tokenizer.save_pretrained(tapt_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e785e",
   "metadata": {},
   "source": [
    "## Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| Transformers:\", __import__(\"transformers\").__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# ================== 1. Paths ==================\n",
    "# Change these paths to your own locations in Drive\n",
    "tapt_dir = \"/content/drive/MyDrive/thesis/models/mlm_roberta_shipping_tapt\"  # saved TAPT model/tokenizer\n",
    "folder_path = \"/content/drive/MyDrive/thesis/shipping_news\"                           # folder containing labeled_sentences1.xlsx\n",
    "\n",
    "assert os.path.isdir(tapt_dir), f\"TAPT dir not found: {tapt_dir}\"\n",
    "xlsx_path = os.path.join(folder_path, \"labeled_sentences1.xlsx\")\n",
    "assert os.path.isfile(xlsx_path), f\"Dataset file not found: {xlsx_path}\"\n",
    "\n",
    "# ================== 2. Data Loading ==================\n",
    "label_df = pd.read_excel(xlsx_path)[['sentence', 'label']].dropna()\n",
    "\n",
    "# Keep max 500 examples per class (you can remove this if you want all data)\n",
    "label_df = label_df.groupby('label').head(500).reset_index(drop=True)\n",
    "label_df['label'] = label_df['label'].astype(int)  # 0=irrelevant, 1=rise, 2=fall\n",
    "\n",
    "id2label = {0: \"irrelevant\", 1: \"rise\", 2: \"fall\"}\n",
    "\n",
    "# Create binary labels for the two-stage setup\n",
    "label_df['label_rel'] = label_df['label'].apply(lambda x: 0 if x == 0 else 1)  # 0=irrelevant, 1=relevant\n",
    "label_df['label_dir'] = label_df['label'].apply(lambda x: 0 if x == 1 else (1 if x == 2 else 0))  # rise/fall\n",
    "\n",
    "# Compute class weights for relevance head\n",
    "rel_counts = Counter(label_df['label_rel'])\n",
    "total = sum(rel_counts.values())\n",
    "class_weights = [total / rel_counts[i] for i in range(2)]\n",
    "print(\"Class counts (rel):\", dict(rel_counts), \"-> weights:\", class_weights)\n",
    "\n",
    "# Build Hugging Face Dataset\n",
    "features = Features({\n",
    "    \"sentence\": Value(\"string\"),\n",
    "    \"label\": ClassLabel(names=[id2label[i] for i in range(3)]),\n",
    "    \"label_rel\": ClassLabel(names=[\"irrelevant\", \"relevant\"]),\n",
    "    \"label_dir\": ClassLabel(names=[\"rise\", \"fall\"])\n",
    "})\n",
    "full_ds = Dataset.from_pandas(label_df, features=features)\n",
    "\n",
    "# Split into train/valid/test with stratification\n",
    "ds = full_ds.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
    "tmp = ds[\"train\"].train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
    "dataset = {\"train\": tmp[\"train\"], \"valid\": tmp[\"test\"], \"test\": ds[\"test\"]}\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    print(k, len(v))\n",
    "\n",
    "# ================== 3. Model Definition ==================\n",
    "class TwoStageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A two-head model:\n",
    "      - relevance head: irrelevant vs relevant\n",
    "      - direction head: rise vs fall (only applied if relevant)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model_name, hidden_size=768, rel_class_weights=None, num_layers_to_freeze=6):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.rel_head = nn.Linear(hidden_size, 2)  # relevance classification\n",
    "        self.dir_head = nn.Linear(hidden_size, 2)  # direction classification\n",
    "\n",
    "        # Freeze the first N encoder layers\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if \"encoder.layer.\" in name:\n",
    "                try:\n",
    "                    layer_num = int(name.split(\"encoder.layer.\")[1].split(\".\")[0])\n",
    "                    if layer_num < num_layers_to_freeze:\n",
    "                        param.requires_grad = False\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Loss functions\n",
    "        if rel_class_weights is not None:\n",
    "            self.rel_loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(rel_class_weights, dtype=torch.float32))\n",
    "        else:\n",
    "            self.rel_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.dir_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,\n",
    "                labels=None,  # not used\n",
    "                labels_rel=None,\n",
    "                labels_dir=None):\n",
    "        enc_out = self.encoder(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls = self.dropout(enc_out[:, 0])  # [CLS] token\n",
    "        logits_rel = self.rel_head(cls)\n",
    "        logits_dir = self.dir_head(cls)\n",
    "\n",
    "        loss = None\n",
    "        if labels_rel is not None:\n",
    "            # relevance loss always computed\n",
    "            loss_rel = self.rel_loss_fn(logits_rel, labels_rel)\n",
    "            loss_dir = 0.0\n",
    "            if labels_dir is not None:\n",
    "                # only compute direction loss for relevant samples\n",
    "                mask = labels_rel == 1\n",
    "                if mask.any():\n",
    "                    loss_dir = self.dir_loss_fn(logits_dir[mask], labels_dir[mask])\n",
    "            loss = loss_rel + loss_dir\n",
    "\n",
    "        return {\"loss\": loss, \"logits_rel\": logits_rel, \"logits_dir\": logits_dir}\n",
    "\n",
    "# ================== 4. Tokenization ==================\n",
    "tokenizer = AutoTokenizer.from_pretrained(tapt_dir)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized = {k: v.map(tokenize, batched=True, remove_columns=[\"sentence\"]) for k, v in dataset.items()}\n",
    "\n",
    "# Rename columns to match trainer inputs\n",
    "for split in tokenized:\n",
    "    tokenized[split] = tokenized[split].rename_column(\"label_rel\", \"labels_rel\")\n",
    "    tokenized[split] = tokenized[split].rename_column(\"label_dir\", \"labels_dir\")\n",
    "    tokenized[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels_rel\", \"labels_dir\", \"label\"])\n",
    "\n",
    "# ================== 5. Custom Trainer ==================\n",
    "class TwoStageTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer that passes two labels to the model\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels_rel = inputs.pop(\"labels_rel\")\n",
    "        labels_dir = inputs.pop(\"labels_dir\")\n",
    "        outputs = model(**inputs, labels_rel=labels_rel, labels_dir=labels_dir)\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ================== 6. Training Arguments ==================\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./two_stage_model\",\n",
    "    eval_strategy=\"epoch\",   # evaluate at each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=torch.cuda.is_available(),   # use mixed precision if GPU\n",
    ")\n",
    "\n",
    "# ================== 7. Model + Custom Optimizer ==================\n",
    "model = TwoStageModel(\n",
    "    base_model_name=tapt_dir,\n",
    "    rel_class_weights=class_weights,\n",
    "    num_layers_to_freeze=9\n",
    ")\n",
    "\n",
    "# Different learning rates for encoder vs classifier heads\n",
    "encoder_params, decoder_params = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if name.startswith(\"encoder.\"):\n",
    "            encoder_params.append(param)\n",
    "        else:\n",
    "            decoder_params.append(param)\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {\"params\": encoder_params, \"lr\": 1e-5},\n",
    "    {\"params\": decoder_params, \"lr\": 3e-5},\n",
    "], weight_decay=0.01)\n",
    "\n",
    "trainer = TwoStageTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.optimizer = optimizer  # inject custom optimizer\n",
    "\n",
    "# ================== 8. Train ==================\n",
    "train_result = trainer.train()\n",
    "print(\"Best model path:\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "# ================== 9. Evaluation Helpers ==================\n",
    "@torch.no_grad()\n",
    "def collect_logits(model, ds, trainer):\n",
    "    \"\"\"Collect logits and labels from a dataset\"\"\"\n",
    "    dl = trainer.get_eval_dataloader(ds)\n",
    "    model.eval()\n",
    "    all_rel, all_dir, all_rel_labels, all_dir_labels = [], [], [], []\n",
    "    device = next(model.parameters()).device\n",
    "    for batch in dl:\n",
    "        labels_rel = batch.pop(\"labels_rel\")\n",
    "        labels_dir = batch.pop(\"labels_dir\")\n",
    "        batch = {k: v.to(device) for k,v in batch.items() if k in [\"input_ids\",\"attention_mask\"]}\n",
    "        out = model(**batch)\n",
    "        all_rel.append(out[\"logits_rel\"].cpu())\n",
    "        all_dir.append(out[\"logits_dir\"].cpu())\n",
    "        all_rel_labels.append(labels_rel.cpu())\n",
    "        all_dir_labels.append(labels_dir.cpu())\n",
    "    rel = torch.cat(all_rel).numpy()\n",
    "    dire = torch.cat(all_dir).numpy()\n",
    "    y_rel = torch.cat(all_rel_labels).numpy()\n",
    "    y_dir = torch.cat(all_dir_labels).numpy()\n",
    "    return rel, dire, y_rel, y_dir\n",
    "\n",
    "def predict_two_stage(model, ds, trainer, tau_rel=0.5):\n",
    "    \"\"\"Two-stage prediction with threshold tau_rel\"\"\"\n",
    "    rel_logits, dir_logits, _, _ = collect_logits(model, ds, trainer)\n",
    "    rel_probs = softmax(torch.tensor(rel_logits), dim=1).numpy()\n",
    "    dir_probs = softmax(torch.tensor(dir_logits), dim=1).numpy()\n",
    "\n",
    "    preds = []\n",
    "    for pr, pd in zip(rel_probs, dir_probs):\n",
    "        if pr[1] < tau_rel:\n",
    "            preds.append(0)  # irrelevant\n",
    "        else:\n",
    "            preds.append(1 if pd[0] >= pd[1] else 2)  # rise/fall\n",
    "    return np.array(preds)\n",
    "\n",
    "# ================== 10. Evaluate on Test Set ==================\n",
    "test_preds = predict_two_stage(model, tokenized[\"test\"], trainer, tau_rel=0.5)\n",
    "y_true = np.array(dataset[\"test\"][\"label\"])\n",
    "print(\"== Test (no calibration) ==\")\n",
    "print(classification_report(y_true, test_preds, target_names=[id2label[i] for i in range(3)], digits=3))\n",
    "print(confusion_matrix(y_true, test_preds))\n",
    "\n",
    "# ================== 11. Temperature Scaling ==================\n",
    "def fit_temperature(logits_np, labels_np, max_iter=50):\n",
    "    \"\"\"Fit temperature scaling using LBFGS\"\"\"\n",
    "    logits = torch.tensor(logits_np, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels_np, dtype=torch.long)\n",
    "    T = torch.nn.Parameter(torch.ones(1, dtype=torch.float32))\n",
    "    optim = torch.optim.LBFGS([T], lr=0.01, max_iter=max_iter)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def closure():\n",
    "        optim.zero_grad()\n",
    "        loss = ce(logits / T, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optim.step(closure)\n",
    "    return float(T.detach().cpu().item())\n",
    "\n",
    "# Fit temperature on validation set\n",
    "rel_logits_val, dir_logits_val, y_rel_val, y_dir_val = collect_logits(model, tokenized[\"valid\"], trainer)\n",
    "T_rel = fit_temperature(rel_logits_val, y_rel_val)\n",
    "mask_val = (y_rel_val == 1)\n",
    "T_dir = fit_temperature(dir_logits_val[mask_val], y_dir_val[mask_val])\n",
    "print(f\"T_rel={T_rel:.3f}, T_dir={T_dir:.3f}\")\n",
    "\n",
    "def predict_two_stage_temp(model, ds, trainer, tau_rel=0.5, T_rel=1.0, T_dir=1.0):\n",
    "    \"\"\"Two-stage prediction with temperature scaling\"\"\"\n",
    "    rel_logits, dir_logits, _, _ = collect_logits(model, ds, trainer)\n",
    "    rel_probs = softmax(torch.tensor(rel_logits)/T_rel, dim=1).numpy()\n",
    "    dir_probs = softmax(torch.tensor(dir_logits)/T_dir, dim=1).numpy()\n",
    "    preds = []\n",
    "    for pr, pd in zip(rel_probs, dir_probs):\n",
    "        if pr[1] < tau_rel:\n",
    "            preds.append(0)\n",
    "        else:\n",
    "            preds.append(1 if pd[0]>=pd[1] else 2)\n",
    "    return np.array(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
