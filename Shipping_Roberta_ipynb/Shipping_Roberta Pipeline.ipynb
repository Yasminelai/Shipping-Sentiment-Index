{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_maritime_corpus():\n",
    "    \"\"\"Load maritime_corpus.csv from Data/ folder\"\"\"\n",
    "    data_path = \"./Data/maritime_corpus.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded maritime_corpus.csv: {len(df)} samples\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def load_news_corpus():\n",
    "    \"\"\"Load News_corpus.csv from Data/ folder\"\"\"\n",
    "    data_path = \"./Data/News_corpus.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded News_corpus.csv: {len(df)} samples\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def load_annotation():\n",
    "    \"\"\"Load Annotation.csv from Data/ folder\"\"\"\n",
    "    data_path = \"./Data/Annotation.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded Annotation.csv: {len(df)} samples\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"transformers>=4.41.0\" \"datasets>=2.19.0\" \"accelerate>=0.30.0\" \"scikit-learn>=1.3.0\" \"pandas>=2.0.0\" \"openpyxl>=3.1.0\" --upgrade\n",
    "\n",
    "import os, random, numpy as np, torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d465407",
   "metadata": {},
   "source": [
    "## DAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0335407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DAPT Data Loading ===\n",
    "maritime_df = load_maritime_corpus()\n",
    "dataset = Dataset.from_pandas(maritime_df)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# === Tokenization ===\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "\n",
    "model_ckpt = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, max_length=128)\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"sentence\"])\n",
    "\n",
    "# === Data Collator ===\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# === Training arguments ===\n",
    "dapt_output_dir = \"./Model/dapt_model\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=dapt_output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir=f\"{dapt_output_dir}/logs\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    resume_from_checkpoint=None,\n",
    ")\n",
    "\n",
    "# === Load model ===\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_ckpt)\n",
    "\n",
    "# === Compute metrics for MLM accuracy ===\n",
    "import numpy as np\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    mask = labels != -100  # Ignore padding positions\n",
    "    correct = (predictions == labels) & mask\n",
    "    accuracy = correct.sum() / mask.sum()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# === Trainer with progress bar ===\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Use all test samples instead of selecting 500\n",
    "small_eval_dataset = tokenized[\"test\"]\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# === Training with progress tracking ===\n",
    "print(\" Starting DAPT training...\")\n",
    "print(f\"Training samples: {len(tokenized['train'])}\")\n",
    "print(f\"Evaluation samples: {len(small_eval_dataset)}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "import os\n",
    "checkpoint_dirs = [d for d in os.listdir(dapt_output_dir) if d.startswith('checkpoint-')] if os.path.exists(dapt_output_dir) else []\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = sorted(checkpoint_dirs)[-1]\n",
    "    print(f\"�� Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\" Training will resume from latest checkpoint\")\n",
    "else:\n",
    "    print(\" No existing checkpoints found, starting fresh training\")\n",
    "\n",
    "# Initial evaluation\n",
    "initial_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "print(\"\\n Initial Evaluation (before training):\")\n",
    "for key, value in initial_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Training with progress bar\n",
    "trainer.train()\n",
    "\n",
    "# Final evaluation\n",
    "final_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "print(\"\\n Final Evaluation (after training):\")\n",
    "for key, value in final_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(dapt_output_dir)\n",
    "tokenizer.save_pretrained(dapt_output_dir)\n",
    "print(f\"\\n DAPT model saved to: {dapt_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f56ad",
   "metadata": {},
   "source": [
    "## TAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a462e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TAPT Data Loading ===\n",
    "news_df = load_news_corpus()\n",
    "dataset = Dataset.from_pandas(news_df)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "def tapt_tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized = dataset.map(tapt_tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# === TAPT TrainingArguments ===\n",
    "tapt_output_dir = \"./Model/tapt_model\"\n",
    "tapt_args = TrainingArguments(\n",
    "    output_dir=tapt_output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    logging_dir=f\"{tapt_output_dir}/logs\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    resume_from_checkpoint=None,\n",
    ")\n",
    "\n",
    "# === Load DAPT model as base ===\n",
    "model = AutoModelForMaskedLM.from_pretrained(dapt_output_dir)\n",
    "\n",
    "# === Accuracy compute_metrics function ===\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    mask = labels != -100\n",
    "    correct = (predictions == labels) & mask\n",
    "    accuracy = correct.sum() / mask.sum()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# === TAPT Trainer ===\n",
    "small_eval_dataset = tokenized[\"test\"]\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=tapt_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# === Training with progress tracking ===\n",
    "print(\"Starting TAPT training...\")\n",
    "print(f\"Training samples: {len(tokenized['train'])}\")\n",
    "print(f\"Evaluation samples: {len(small_eval_dataset)}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "import os\n",
    "checkpoint_dirs = [d for d in os.listdir(tapt_output_dir) if d.startswith('checkpoint-')] if os.path.exists(tapt_output_dir) else []\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = sorted(checkpoint_dirs)[-1]\n",
    "    print(f\"Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Training will resume from latest checkpoint\")\n",
    "else:\n",
    "    print(\"No existing checkpoints found, starting fresh training\")\n",
    "\n",
    "# Initial evaluation\n",
    "initial_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "print(\"\\nInitial Evaluation (before training):\")\n",
    "for key, value in initial_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Training with progress bar\n",
    "trainer.train()\n",
    "\n",
    "# Final evaluation\n",
    "final_metrics = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
    "print(\"\\nFinal Evaluation (after training):\")\n",
    "for key, value in final_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(tapt_output_dir)\n",
    "tokenizer.save_pretrained(tapt_output_dir)\n",
    "print(f\"\\nTAPT model saved to: {tapt_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e785e",
   "metadata": {},
   "source": [
    "## Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Classifier Data Loading ===\n",
    "annotation_df = load_annotation()\n",
    "\n",
    "# Keep max 500 examples per class (you can remove this if you want all data)\n",
    "label_df = annotation_df.groupby('label').head(500).reset_index(drop=True)\n",
    "label_df['label'] = label_df['label'].astype(int)  # 0=irrelevant, 1=rise, 2=fall\n",
    "\n",
    "id2label = {0: \"irrelevant\", 1: \"rise\", 2: \"fall\"}\n",
    "\n",
    "# Create binary labels for the two-stage setup\n",
    "label_df['label_rel'] = label_df['label'].apply(lambda x: 0 if x == 0 else 1)  # 0=irrelevant, 1=relevant\n",
    "label_df['label_dir'] = label_df['label'].apply(lambda x: 0 if x == 1 else (1 if x == 2 else 0))  # rise/fall\n",
    "\n",
    "# Compute class weights for relevance head\n",
    "rel_counts = Counter(label_df['label_rel'])\n",
    "total = sum(rel_counts.values())\n",
    "class_weights = [total / rel_counts[i] for i in range(2)]\n",
    "print(\"Class counts (rel):\", dict(rel_counts), \"-> weights:\", class_weights)\n",
    "\n",
    "# Build Hugging Face Dataset\n",
    "features = Features({\n",
    "    \"sentence\": Value(\"string\"),\n",
    "    \"label\": ClassLabel(names=[id2label[i] for i in range(3)]),\n",
    "    \"label_rel\": ClassLabel(names=[\"irrelevant\", \"relevant\"]),\n",
    "    \"label_dir\": ClassLabel(names=[\"rise\", \"fall\"])\n",
    "})\n",
    "full_ds = Dataset.from_pandas(label_df, features=features)\n",
    "\n",
    "# Split into train/valid/test with stratification\n",
    "ds = full_ds.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
    "tmp = ds[\"train\"].train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
    "dataset = {\"train\": tmp[\"train\"], \"valid\": tmp[\"test\"], \"test\": ds[\"test\"]}\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    print(f\"{k}: {len(v)} samples\")\n",
    "\n",
    "# === Reproducibility ===\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| Transformers:\", __import__(\"transformers\").__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fda291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Definition ===\n",
    "class TwoStageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A two-head model:\n",
    "      - relevance head: irrelevant vs relevant\n",
    "      - direction head: rise vs fall (only applied if relevant)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model_name, hidden_size=768, rel_class_weights=None, num_layers_to_freeze=6):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.rel_head = nn.Linear(hidden_size, 2)  # relevance classification\n",
    "        self.dir_head = nn.Linear(hidden_size, 2)  # direction classification\n",
    "\n",
    "        # Freeze the first N encoder layers\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if \"encoder.layer.\" in name:\n",
    "                try:\n",
    "                    layer_num = int(name.split(\"encoder.layer.\")[1].split(\".\")[0])\n",
    "                    if layer_num < num_layers_to_freeze:\n",
    "                        param.requires_grad = False\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Loss functions\n",
    "        if rel_class_weights is not None:\n",
    "            self.rel_loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(rel_class_weights, dtype=torch.float32))\n",
    "        else:\n",
    "            self.rel_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.dir_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,\n",
    "                labels=None,  # not used\n",
    "                labels_rel=None,\n",
    "                labels_dir=None):\n",
    "        enc_out = self.encoder(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        cls = self.dropout(enc_out[:, 0])  # [CLS] token\n",
    "        logits_rel = self.rel_head(cls)\n",
    "        logits_dir = self.dir_head(cls)\n",
    "\n",
    "        loss = None\n",
    "        if labels_rel is not None:\n",
    "            # relevance loss always computed\n",
    "            loss_rel = self.rel_loss_fn(logits_rel, labels_rel)\n",
    "            loss_dir = 0.0\n",
    "            if labels_dir is not None:\n",
    "                # only compute direction loss for relevant samples\n",
    "                mask = labels_rel == 1\n",
    "                if mask.any():\n",
    "                    loss_dir = self.dir_loss_fn(logits_dir[mask], labels_dir[mask])\n",
    "            loss = loss_rel + loss_dir\n",
    "\n",
    "        return {\"loss\": loss, \"logits_rel\": logits_rel, \"logits_dir\": logits_dir}\n",
    "\n",
    "# === Tokenization ===\n",
    "tapt_dir = \"./Model/tapt_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tapt_dir)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized = {k: v.map(tokenize, batched=True, remove_columns=[\"sentence\"]) for k, v in dataset.items()}\n",
    "\n",
    "# Rename columns to match trainer inputs\n",
    "for split in tokenized:\n",
    "    tokenized[split] = tokenized[split].rename_column(\"label_rel\", \"labels_rel\")\n",
    "    tokenized[split] = tokenized[split].rename_column(\"label_dir\", \"labels_dir\")\n",
    "    tokenized[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels_rel\", \"labels_dir\", \"label\"])\n",
    "\n",
    "# === Custom Trainer ===\n",
    "class TwoStageTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer that passes two labels to the model\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels_rel = inputs.pop(\"labels_rel\")\n",
    "        labels_dir = inputs.pop(\"labels_dir\")\n",
    "        outputs = model(**inputs, labels_rel=labels_rel, labels_dir=labels_dir)\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# === Training Arguments ===\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./Model/classifier\",\n",
    "    eval_strategy=\"epoch\",   # evaluate at each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=torch.cuda.is_available(),   # use mixed precision if GPU\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    # Checkpoint resume settings\n",
    "    resume_from_checkpoint=None,  # Set to checkpoint path to resume, or None for auto-resume\n",
    ")\n",
    "\n",
    "# === Model + Custom Optimizer ===\n",
    "model = TwoStageModel(\n",
    "    base_model_name=tapt_dir,\n",
    "    rel_class_weights=class_weights,\n",
    "    num_layers_to_freeze=9\n",
    ")\n",
    "\n",
    "# Different learning rates for encoder vs classifier heads\n",
    "encoder_params, classifier_params = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if name.startswith(\"encoder.\"):\n",
    "            encoder_params.append(param)\n",
    "        else:\n",
    "            classifier_params.append(param)  # These are the classification heads\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {\"params\": encoder_params, \"lr\": 1e-5},\n",
    "    {\"params\": classifier_params, \"lr\": 3e-5},  # Higher LR for classifier heads\n",
    "], weight_decay=0.01)\n",
    "\n",
    "trainer = TwoStageTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.optimizer = optimizer  # inject custom optimizer\n",
    "\n",
    "# === Training with progress tracking ===\n",
    "print(\"Starting Classifier training...\")\n",
    "print(f\"Training samples: {len(tokenized['train'])}\")\n",
    "print(f\"Validation samples: {len(tokenized['valid'])}\")\n",
    "print(f\"Test samples: {len(tokenized['test'])}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "import os\n",
    "classifier_output_dir = \"./Model/classifier\"\n",
    "checkpoint_dirs = [d for d in os.listdir(classifier_output_dir) if d.startswith('checkpoint-')] if os.path.exists(classifier_output_dir) else []\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = sorted(checkpoint_dirs)[-1]\n",
    "    print(f\"Found existing checkpoint: {latest_checkpoint}\")\n",
    "    print(\"Training will resume from latest checkpoint\")\n",
    "else:\n",
    "    print(\"No existing checkpoints found, starting fresh training\")\n",
    "\n",
    "# Initial evaluation\n",
    "initial_metrics = trainer.evaluate(eval_dataset=tokenized[\"valid\"])\n",
    "print(\"\\nInitial Evaluation (before training):\")\n",
    "for key, value in initial_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Training with progress bar\n",
    "train_result = trainer.train()\n",
    "print(\"Best model path:\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "# Final evaluation\n",
    "final_metrics = trainer.evaluate(eval_dataset=tokenized[\"valid\"])\n",
    "print(\"\\nFinal Evaluation (after training):\")\n",
    "for key, value in final_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nClassifier model saved to: ./Model/classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation Helpers ===\n",
    "@torch.no_grad()\n",
    "def collect_logits(model, ds, trainer):\n",
    "    \"\"\"Collect logits and labels from a dataset\"\"\"\n",
    "    dl = trainer.get_eval_dataloader(ds)\n",
    "    model.eval()\n",
    "    all_rel, all_dir, all_rel_labels, all_dir_labels = [], [], [], []\n",
    "    device = next(model.parameters()).device\n",
    "    for batch in dl:\n",
    "        labels_rel = batch.pop(\"labels_rel\")\n",
    "        labels_dir = batch.pop(\"labels_dir\")\n",
    "        batch = {k: v.to(device) for k,v in batch.items() if k in [\"input_ids\",\"attention_mask\"]}\n",
    "        out = model(**batch)\n",
    "        all_rel.append(out[\"logits_rel\"].cpu())\n",
    "        all_dir.append(out[\"logits_dir\"].cpu())\n",
    "        all_rel_labels.append(labels_rel.cpu())\n",
    "        all_dir_labels.append(labels_dir.cpu())\n",
    "    rel = torch.cat(all_rel).numpy()\n",
    "    dire = torch.cat(all_dir).numpy()\n",
    "    y_rel = torch.cat(all_rel_labels).numpy()\n",
    "    y_dir = torch.cat(all_dir_labels).numpy()\n",
    "    return rel, dire, y_rel, y_dir\n",
    "\n",
    "def predict_two_stage(model, ds, trainer, tau_rel=0.5):\n",
    "    \"\"\"Two-stage prediction with threshold tau_rel\"\"\"\n",
    "    rel_logits, dir_logits, _, _ = collect_logits(model, ds, trainer)\n",
    "    rel_probs = softmax(torch.tensor(rel_logits), dim=1).numpy()\n",
    "    dir_probs = softmax(torch.tensor(dir_logits), dim=1).numpy()\n",
    "\n",
    "    preds = []\n",
    "    for pr, pd in zip(rel_probs, dir_probs):\n",
    "        if pr[1] < tau_rel:\n",
    "            preds.append(0)  # irrelevant\n",
    "        else:\n",
    "            preds.append(1 if pd[0] >= pd[1] else 2)  # rise/fall\n",
    "    return np.array(preds)\n",
    "\n",
    "# === Evaluate on Test Set ===\n",
    "test_preds = predict_two_stage(model, tokenized[\"test\"], trainer, tau_rel=0.5)\n",
    "y_true = np.array(dataset[\"test\"][\"label\"])\n",
    "print(\"== Test Results (no calibration) ==\")\n",
    "print(classification_report(y_true, test_preds, target_names=[id2label[i] for i in range(3)], digits=3))\n",
    "print(confusion_matrix(y_true, test_preds))\n",
    "\n",
    "# === Temperature Scaling ===\n",
    "def fit_temperature(logits_np, labels_np, max_iter=50):\n",
    "    \"\"\"Fit temperature scaling using LBFGS\"\"\"\n",
    "    logits = torch.tensor(logits_np, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels_np, dtype=torch.long)\n",
    "    T = torch.nn.Parameter(torch.ones(1, dtype=torch.float32))\n",
    "    optim = torch.optim.LBFGS([T], lr=0.01, max_iter=max_iter)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def closure():\n",
    "        optim.zero_grad()\n",
    "        loss = ce(logits / T, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optim.step(closure)\n",
    "    return float(T.detach().cpu().item())\n",
    "\n",
    "# Fit temperature on validation set\n",
    "rel_logits_val, dir_logits_val, y_rel_val, y_dir_val = collect_logits(model, tokenized[\"valid\"], trainer)\n",
    "T_rel = fit_temperature(rel_logits_val, y_rel_val)\n",
    "mask_val = (y_rel_val == 1)\n",
    "T_dir = fit_temperature(dir_logits_val[mask_val], y_dir_val[mask_val])\n",
    "print(f\"Temperature scaling: T_rel={T_rel:.3f}, T_dir={T_dir:.3f}\")\n",
    "\n",
    "def predict_two_stage_temp(model, ds, trainer, tau_rel=0.5, T_rel=1.0, T_dir=1.0):\n",
    "    \"\"\"Two-stage prediction with temperature scaling\"\"\"\n",
    "    rel_logits, dir_logits, _, _ = collect_logits(model, ds, trainer)\n",
    "    rel_probs = softmax(torch.tensor(rel_logits)/T_rel, dim=1).numpy()\n",
    "    dir_probs = softmax(torch.tensor(dir_logits)/T_dir, dim=1).numpy()\n",
    "    preds = []\n",
    "    for pr, pd in zip(rel_probs, dir_probs):\n",
    "        if pr[1] < tau_rel:\n",
    "            preds.append(0)\n",
    "        else:\n",
    "            preds.append(1 if pd[0]>=pd[1] else 2)\n",
    "    return np.array(preds)\n",
    "\n",
    "# === Final Test Results with Temperature Scaling ===\n",
    "test_preds_temp = predict_two_stage_temp(model, tokenized[\"test\"], trainer, tau_rel=0.5, T_rel=T_rel, T_dir=T_dir)\n",
    "print(\"\\n== Test Results (with temperature scaling) ==\")\n",
    "print(classification_report(y_true, test_preds_temp, target_names=[id2label[i] for i in range(3)], digits=3))\n",
    "print(confusion_matrix(y_true, test_preds_temp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e4534",
   "metadata": {},
   "source": [
    "## Manual Checkpoint Resume Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ca1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you need to resume from a specific checkpoint instead of auto-resume:\n",
    "\n",
    "1. For DAPT training:\n",
    "   training_args.resume_from_checkpoint = \"./Model/dapt_model/checkpoint-1000\"\n",
    "   \n",
    "2. For TAPT training:\n",
    "   tapt_args.resume_from_checkpoint = \"./Model/tapt_model/checkpoint-500\"\n",
    "   \n",
    "3. For Classifier training:\n",
    "   args.resume_from_checkpoint = \"./Model/classifier/checkpoint-2\"\n",
    "\n",
    "Checkpoint files are automatically saved at:\n",
    "- DAPT: every 1000 steps\n",
    "- TAPT: every 100 steps  \n",
    "- Classifier: every epoch\n",
    "\n",
    "To list available checkpoints:\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "def list_checkpoints(model_dir):\n",
    "    \"\"\"List all available checkpoints in a model directory\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"Directory {model_dir} does not exist\")\n",
    "        return []\n",
    "    \n",
    "    checkpoints = [d for d in os.listdir(model_dir) if d.startswith('checkpoint-')]\n",
    "    if checkpoints:\n",
    "        checkpoints.sort(key=lambda x: int(x.split('-')[1]))\n",
    "        print(f\"Available checkpoints in {model_dir}:\")\n",
    "        for ckpt in checkpoints:\n",
    "            print(f\"  - {ckpt}\")\n",
    "    else:\n",
    "        print(f\"No checkpoints found in {model_dir}\")\n",
    "    \n",
    "    return checkpoints\n",
    "\n",
    "print(\"=== Checkpoint Status ===\")\n",
    "dapt_checkpoints = list_checkpoints(\"./Model/dapt_model\")\n",
    "tapt_checkpoints = list_checkpoints(\"./Model/tapt_model\") \n",
    "classifier_checkpoints = list_checkpoints(\"./Model/classifier\")\n",
    "\n",
    "print(\"\\n Tip: Set resume_from_checkpoint to any of the above paths to resume from that specific checkpoint\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipping-roberta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
